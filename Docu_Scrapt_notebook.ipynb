{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docu_Manim_Scrap.py\n",
    "\n",
    "**26 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment.\n",
      "Collecting links from the documentation...\n",
      "Found 482 links.\n",
      "Downloading pages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading HTML pages:  49%|████▉     | 237/482 [04:20<04:28,  1.10s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 174\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning in local environment.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 174\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[4], line 150\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    148\u001b[0m html_files \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m tqdm(links, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading HTML pages\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 150\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m download_page(link, OUTPUT_DIR, progress)\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_path:\n\u001b[0;32m    152\u001b[0m         html_files\u001b[38;5;241m.\u001b[39mappend(file_path)\n",
      "Cell \u001b[1;32mIn[4], line 102\u001b[0m, in \u001b[0;36mdownload_page\u001b[1;34m(url, output_dir, progress)\u001b[0m\n\u001b[0;32m     99\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# Convert links to absolute for proper functionality\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     html_content \u001b[38;5;241m=\u001b[39m make_links_absolute(response\u001b[38;5;241m.\u001b[39mtext, url)\n\u001b[0;32m    103\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m url\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 75\u001b[0m, in \u001b[0;36mmake_links_absolute\u001b[1;34m(html_content, base_url)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_links_absolute\u001b[39m(html_content, base_url):\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    Converts all relative links in the HTML content to absolute URLs.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m        str: The updated HTML content with absolute links.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html_content, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m], href\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     77\u001b[0m         tag[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m urljoin(base_url, tag[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\bs4\\__init__.py:335\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed()\n\u001b[0;32m    336\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\bs4\\__init__.py:478\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# Convert the document to Unicode.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m--> 478\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mfeed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarkup)\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\bs4\\builder\\_htmlparser.py:380\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    378\u001b[0m parser\u001b[38;5;241m.\u001b[39msoup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     parser\u001b[38;5;241m.\u001b[39mfeed(markup)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# when there's an error in the doctype declaration.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParserRejectedMarkup(e)\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\html\\parser.py:111\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m+\u001b[39m data\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoahead(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\html\\parser.py:171\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m startswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m, i):\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m starttagopen\u001b[38;5;241m.\u001b[39mmatch(rawdata, i): \u001b[38;5;66;03m# < + letter\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_starttag(i)\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[0;32m    173\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_endtag(i)\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\html\\parser.py:338\u001b[0m, in \u001b[0;36mHTMLParser.parse_starttag\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_startendtag(tag, attrs)\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_starttag(tag, attrs)\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCDATA_CONTENT_ELEMENTS:\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_cdata_mode(tag)\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\bs4\\builder\\_htmlparser.py:137\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_starttag\u001b[1;34m(self, name, attrs, handle_empty_element)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m#print(\"START\", name)\u001b[39;00m\n\u001b[0;32m    136\u001b[0m sourceline, sourcepos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetpos()\n\u001b[1;32m--> 137\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\u001b[38;5;241m.\u001b[39mhandle_starttag(\n\u001b[0;32m    138\u001b[0m     name, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, attr_dict, sourceline\u001b[38;5;241m=\u001b[39msourceline,\n\u001b[0;32m    139\u001b[0m     sourcepos\u001b[38;5;241m=\u001b[39msourcepos\n\u001b[0;32m    140\u001b[0m )\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mand\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mis_empty_element \u001b[38;5;129;01mand\u001b[39;00m handle_empty_element:\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;66;03m# Unlike other parsers, html.parser doesn't send separate end tag\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# events for empty-element tags. (It's handled in\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# don't want handle_endtag() to cross off any previous end\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# events for tags of this name.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_endtag(name, check_already_closed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\bs4\\__init__.py:760\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_starttag\u001b[1;34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos, namespaces)\u001b[0m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_most_recent_element\u001b[38;5;241m.\u001b[39mnext_element \u001b[38;5;241m=\u001b[39m tag\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_most_recent_element \u001b[38;5;241m=\u001b[39m tag\n\u001b[1;32m--> 760\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpushTag(tag)\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tag\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\MiguelEnvHaB\\Lib\\site-packages\\bs4\\__init__.py:572\u001b[0m, in \u001b[0;36mBeautifulSoup.pushTag\u001b[1;34m(self, tag)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentTag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagStack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentTag\n\u001b[1;32m--> 572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpushTag\u001b[39m(\u001b[38;5;28mself\u001b[39m, tag):\n\u001b[0;32m    573\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Internal method called by handle_starttag when a tag is opened.\"\"\"\u001b[39;00m\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;66;03m#print(\"Push\", tag.name)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from weasyprint import HTML\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Detect if the environment is Google Colab or local\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    COLAB_ENV = True\n",
    "except ImportError:\n",
    "    COLAB_ENV = False\n",
    "\n",
    "# Base URL of the Manim documentation\n",
    "BASE_URL = \"https://docs.manim.community/en/stable/reference.html\"\n",
    "\n",
    "# Set output directory based on environment\n",
    "if COLAB_ENV:\n",
    "    OUTPUT_DIR = \"/content/manim_docs\"\n",
    "else:\n",
    "    OUTPUT_DIR = os.path.join(os.getcwd(), \"manim_docs\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# File to track downloaded links\n",
    "PROGRESS_FILE = os.path.join(OUTPUT_DIR, \"progress.json\")\n",
    "\n",
    "# Load or initialize progress tracking\n",
    "def load_progress():\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, \"r\") as file:\n",
    "            return json.load(file)\n",
    "    return {\"downloaded\": []}\n",
    "\n",
    "def save_progress(progress):\n",
    "    with open(PROGRESS_FILE, \"w\") as file:\n",
    "        json.dump(progress, file)\n",
    "\n",
    "def get_links(base_url):\n",
    "    \"\"\"\n",
    "    Collects all unique links from the main reference page.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the documentation.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique absolute URLs found on the page.\n",
    "    \"\"\"\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch {base_url}: {response.status_code}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        url = urljoin(base_url, a_tag['href'])\n",
    "        if \"https://docs.manim.community/en/stable/\" in url:\n",
    "            links.append(url)\n",
    "    return list(set(links))  # Remove duplicates\n",
    "\n",
    "def make_links_absolute(html_content, base_url):\n",
    "    \"\"\"\n",
    "    Converts all relative links in the HTML content to absolute URLs.\n",
    "\n",
    "    Args:\n",
    "        html_content (str): The HTML content as a string.\n",
    "        base_url (str): The base URL to resolve relative links.\n",
    "\n",
    "    Returns:\n",
    "        str: The updated HTML content with absolute links.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    for tag in soup.find_all(['a', 'img'], href=True):\n",
    "        tag['href'] = urljoin(base_url, tag['href'])\n",
    "    for tag in soup.find_all('img', src=True):\n",
    "        tag['src'] = urljoin(base_url, tag['src'])\n",
    "    return str(soup)\n",
    "\n",
    "def download_page(url, output_dir, progress):\n",
    "    \"\"\"\n",
    "    Downloads the content of a given URL and saves it as an HTML file.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to download.\n",
    "        output_dir (str): The directory to save the HTML file.\n",
    "        progress (dict): The progress tracker dictionary.\n",
    "\n",
    "    Returns:\n",
    "        str: The file path of the saved HTML file, or None if the download fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Skip if already downloaded\n",
    "        if url in progress[\"downloaded\"]:\n",
    "            return None\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # Convert links to absolute for proper functionality\n",
    "            html_content = make_links_absolute(response.text, url)\n",
    "            file_name = url.split('/')[-1] or \"index.html\"\n",
    "            file_path = os.path.join(output_dir, f\"{file_name}.html\")\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(html_content)\n",
    "            progress[\"downloaded\"].append(url)\n",
    "            save_progress(progress)\n",
    "            return file_path\n",
    "        else:\n",
    "            print(f\"Failed to download {url}: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def html_to_pdf(html_files, output_pdf):\n",
    "    \"\"\"\n",
    "    Converts a list of HTML files into a single PDF file.\n",
    "\n",
    "    Args:\n",
    "        html_files (list): List of file paths to HTML files.\n",
    "        output_pdf (str): The path to the output PDF file.\n",
    "    \"\"\"\n",
    "    pdf_pages = []\n",
    "    for html_file in tqdm(html_files, desc=\"Rendering HTML to PDF\"):\n",
    "        try:\n",
    "            pdf_pages.append(HTML(html_file, base_url=BASE_URL).render())\n",
    "        except Exception as e:\n",
    "            print(f\"Error rendering {html_file}: {e}\")\n",
    "    if pdf_pages:\n",
    "        combined_pdf = pdf_pages[0]\n",
    "        for page in pdf_pages[1:]:\n",
    "            combined_pdf.pages.extend(page.pages)\n",
    "        combined_pdf.write_pdf(output_pdf)\n",
    "        print(f\"Final PDF generated: {output_pdf}\")\n",
    "    else:\n",
    "        print(\"No PDF pages were generated.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to scrape the Manim documentation and export it as a PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Collect all relevant links\n",
    "        print(\"Collecting links from the documentation...\")\n",
    "        links = get_links(BASE_URL)\n",
    "        print(f\"Found {len(links)} links.\")\n",
    "\n",
    "        # Step 2: Load progress\n",
    "        progress = load_progress()\n",
    "\n",
    "        # Step 3: Download all pages with a progress bar\n",
    "        print(\"Downloading pages...\")\n",
    "        html_files = []\n",
    "        for link in tqdm(links, desc=\"Downloading HTML pages\"):\n",
    "            file_path = download_page(link, OUTPUT_DIR, progress)\n",
    "            if file_path:\n",
    "                html_files.append(file_path)\n",
    "            time.sleep(0.5)  # Adjusted limit\n",
    "\n",
    "        # Step 4: Convert HTML files to a single PDF\n",
    "        print(\"Generating PDF...\")\n",
    "        pdf_path = os.path.join(OUTPUT_DIR, \"manim_docs_complete.pdf\")\n",
    "        html_to_pdf([os.path.join(OUTPUT_DIR, f) for f in os.listdir(OUTPUT_DIR) if f.endswith('.html')], pdf_path)\n",
    "\n",
    "        # Step 5: Handle output for Colab\n",
    "        if COLAB_ENV:\n",
    "            from google.colab import files\n",
    "            print(\"Downloading PDF to local machine...\")\n",
    "            files.download(pdf_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if COLAB_ENV:\n",
    "        print(\"Running in Google Colab environment.\")\n",
    "    else:\n",
    "        print(\"Running in local environment.\")\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiguelEnvHaB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
