{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8npWY8A8XpRLcFO03c1nB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiguelDiLalla/Media_Generators/blob/main/notebooks/Colab_WebScrap_Docu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab_Docu_Scrap.py"
      ],
      "metadata": {
        "id": "kc44Mduc1Ify"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: install dependencies. weasyprint...\n",
        "\n",
        "!pip install weasyprint\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyqopGM21fEe",
        "outputId": "018f4f4e-0b34-4fb0-f801-f43577dbb843"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting weasyprint\n",
            "  Downloading weasyprint-63.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting pydyf>=0.11.0 (from weasyprint)\n",
            "  Downloading pydyf-0.11.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.11/dist-packages (from weasyprint) (1.17.1)\n",
            "Collecting tinyhtml5>=2.0.0b1 (from weasyprint)\n",
            "  Downloading tinyhtml5-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: tinycss2>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from weasyprint) (1.4.0)\n",
            "Collecting cssselect2>=0.1 (from weasyprint)\n",
            "  Downloading cssselect2-0.7.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting Pyphen>=0.9.1 (from weasyprint)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1.0 in /usr/local/lib/python3.11/dist-packages (from weasyprint) (11.1.0)\n",
            "Requirement already satisfied: fonttools>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from fonttools[woff]>=4.0.0->weasyprint) (4.55.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=0.6->weasyprint) (2.22)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from cssselect2>=0.1->weasyprint) (0.5.1)\n",
            "Collecting brotli>=1.0.1 (from fonttools[woff]>=4.0.0->weasyprint)\n",
            "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting zopfli>=0.1.4 (from fonttools[woff]>=4.0.0->weasyprint)\n",
            "  Downloading zopfli-0.2.3.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Downloading weasyprint-63.1-py3-none-any.whl (299 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect2-0.7.0-py3-none-any.whl (15 kB)\n",
            "Downloading pydyf-0.11.0-py3-none-any.whl (8.1 kB)\n",
            "Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tinyhtml5-2.0.0-py3-none-any.whl (39 kB)\n",
            "Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zopfli-0.2.3.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (850 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m850.6/850.6 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brotli, zopfli, tinyhtml5, Pyphen, pydyf, cssselect2, weasyprint\n",
            "Successfully installed Pyphen-0.17.2 brotli-1.1.0 cssselect2-0.7.0 pydyf-0.11.0 tinyhtml5-2.0.0 weasyprint-63.1 zopfli-0.2.3.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri8FymX31G0K",
        "outputId": "75db230b-a2e6-440d-a2b6-6d8031ef97e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab environment.\n",
            "Collecting links from the documentation...\n",
            "Found 482 links within depth 1.\n",
            "Downloading pages...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading HTML pages:  70%|██████▉   | 336/482 [04:24<01:53,  1.29it/s]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from weasyprint import HTML\n",
        "from urllib.parse import urljoin\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Detect if the environment is Google Colab or local\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    COLAB_ENV = False\n",
        "\n",
        "# Base URL of the Manim documentation\n",
        "BASE_URL = \"https://docs.manim.community/en/stable/reference.html\"\n",
        "\n",
        "# Set output directory based on environment\n",
        "if COLAB_ENV:\n",
        "    OUTPUT_DIR = \"/content/manim_docs\"\n",
        "else:\n",
        "    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"manim_docs\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# File to track downloaded links\n",
        "PROGRESS_FILE = os.path.join(OUTPUT_DIR, \"progress.json\")\n",
        "\n",
        "# Maximum depth for scraping\n",
        "MAX_DEPTH = 1\n",
        "\n",
        "# Load or initialize progress tracking\n",
        "def load_progress():\n",
        "    if os.path.exists(PROGRESS_FILE):\n",
        "        with open(PROGRESS_FILE, \"r\") as file:\n",
        "            return json.load(file)\n",
        "    return {\"downloaded\": [], \"visited\": []}\n",
        "\n",
        "def save_progress(progress):\n",
        "    with open(PROGRESS_FILE, \"w\") as file:\n",
        "        json.dump(progress, file)\n",
        "\n",
        "def get_links(base_url, current_depth):\n",
        "    \"\"\"\n",
        "    Collects all unique links from the given page up to a specified depth.\n",
        "\n",
        "    Args:\n",
        "        base_url (str): The base URL of the documentation.\n",
        "        current_depth (int): Current depth of scraping.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of unique absolute URLs found on the page.\n",
        "    \"\"\"\n",
        "    if current_depth > MAX_DEPTH:\n",
        "        return []\n",
        "\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Failed to fetch {base_url}: {response.status_code}\")\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    links = []\n",
        "    for a_tag in soup.find_all('a', href=True):\n",
        "        url = urljoin(base_url, a_tag['href'])\n",
        "        if \"https://docs.manim.community/en/stable/\" in url:\n",
        "            links.append(url)\n",
        "    return list(set(links))  # Remove duplicates\n",
        "\n",
        "def make_links_absolute(html_content, base_url):\n",
        "    \"\"\"\n",
        "    Converts all relative links in the HTML content to absolute URLs.\n",
        "\n",
        "    Args:\n",
        "        html_content (str): The HTML content as a string.\n",
        "        base_url (str): The base URL to resolve relative links.\n",
        "\n",
        "    Returns:\n",
        "        str: The updated HTML content with absolute links.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    for tag in soup.find_all(['a', 'img'], href=True):\n",
        "        tag['href'] = urljoin(base_url, tag['href'])\n",
        "    for tag in soup.find_all('img', src=True):\n",
        "        tag['src'] = urljoin(base_url, tag['src'])\n",
        "    return str(soup)\n",
        "\n",
        "def download_page(url, output_dir, progress):\n",
        "    \"\"\"\n",
        "    Downloads the content of a given URL and saves it as an HTML file.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL to download.\n",
        "        output_dir (str): The directory to save the HTML file.\n",
        "        progress (dict): The progress tracker dictionary.\n",
        "\n",
        "    Returns:\n",
        "        str: The file path of the saved HTML file, or None if the download fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Skip if already downloaded\n",
        "        if url in progress[\"downloaded\"]:\n",
        "            return None\n",
        "\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            # Convert links to absolute for proper functionality\n",
        "            html_content = make_links_absolute(response.text, url)\n",
        "            file_name = url.split('/')[-1] or \"index.html\"\n",
        "            file_path = os.path.join(output_dir, f\"{file_name}.html\")\n",
        "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "                file.write(html_content)\n",
        "            progress[\"downloaded\"].append(url)\n",
        "            save_progress(progress)\n",
        "            return file_path\n",
        "        else:\n",
        "            print(f\"Failed to download {url}: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {url}: {e}\")\n",
        "    return None\n",
        "\n",
        "def html_to_pdf(html_files, output_pdf):\n",
        "    \"\"\"\n",
        "    Converts a list of HTML files into a single PDF file.\n",
        "\n",
        "    Args:\n",
        "        html_files (list): List of file paths to HTML files.\n",
        "        output_pdf (str): The path to the output PDF file.\n",
        "    \"\"\"\n",
        "    pdf_pages = []\n",
        "    for html_file in tqdm(html_files, desc=\"Rendering HTML to PDF\"):\n",
        "        try:\n",
        "            pdf_pages.append(HTML(html_file).render())\n",
        "        except Exception as e:\n",
        "            print(f\"Error rendering {html_file}: {e}\")\n",
        "    if pdf_pages:\n",
        "        combined_pdf = pdf_pages[0]\n",
        "        for page in pdf_pages[1:]:\n",
        "            combined_pdf.pages.extend(page.pages)\n",
        "        combined_pdf.write_pdf(output_pdf)\n",
        "        print(f\"Final PDF generated: {output_pdf}\")\n",
        "    else:\n",
        "        print(\"No PDF pages were generated.\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to scrape the Manim documentation and export it as a PDF.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Collect all relevant links\n",
        "        print(\"Collecting links from the documentation...\")\n",
        "        progress = load_progress()\n",
        "        if \"visited\" not in progress:\n",
        "            progress[\"visited\"] = []\n",
        "\n",
        "        links_to_visit = [BASE_URL]\n",
        "        all_links = set()\n",
        "\n",
        "        for depth in range(1, MAX_DEPTH + 1):\n",
        "            new_links = []\n",
        "            for link in links_to_visit:\n",
        "                if link not in progress[\"visited\"]:\n",
        "                    progress[\"visited\"].append(link)\n",
        "                    save_progress(progress)\n",
        "                    new_links.extend(get_links(link, depth))\n",
        "            all_links.update(new_links)\n",
        "            links_to_visit = new_links\n",
        "\n",
        "        print(f\"Found {len(all_links)} links within depth {MAX_DEPTH}.\")\n",
        "\n",
        "        # Step 2: Download all pages with a progress bar\n",
        "        print(\"Downloading pages...\")\n",
        "        html_files = []\n",
        "        for link in tqdm(all_links, desc=\"Downloading HTML pages\"):\n",
        "            file_path = download_page(link, OUTPUT_DIR, progress)\n",
        "            if file_path:\n",
        "                html_files.append(file_path)\n",
        "            time.sleep(0.5)  # Adjusted limit\n",
        "\n",
        "        # Step 3: Convert HTML files to a single PDF\n",
        "        print(\"Generating PDF...\")\n",
        "        pdf_path = os.path.join(OUTPUT_DIR, \"manim_docs_complete.pdf\")\n",
        "        html_to_pdf([os.path.join(OUTPUT_DIR, f) for f in os.listdir(OUTPUT_DIR) if f.endswith('.html')], pdf_path)\n",
        "\n",
        "        # Step 4: Handle output for Colab\n",
        "        if COLAB_ENV:\n",
        "            from google.colab import files\n",
        "            print(\"Downloading PDF to local machine...\")\n",
        "            files.download(pdf_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if COLAB_ENV:\n",
        "        print(\"Running in Google Colab environment.\")\n",
        "    else:\n",
        "        print(\"Running in local environment.\")\n",
        "    main()\n"
      ]
    }
  ]
}