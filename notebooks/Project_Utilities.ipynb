{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docu_Manim_Scrap.py\n",
    "\n",
    "**26 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from weasyprint import HTML\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Detect if the environment is Google Colab or local\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    COLAB_ENV = True\n",
    "except ImportError:\n",
    "    COLAB_ENV = False\n",
    "\n",
    "# Base URL of the Manim documentation\n",
    "BASE_URL = \"https://docs.manim.community/en/stable/reference.html\"\n",
    "\n",
    "# Set output directory based on environment\n",
    "if COLAB_ENV:\n",
    "    OUTPUT_DIR = \"/content/manim_docs\"\n",
    "else:\n",
    "    OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"manim_docs\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# File to track downloaded links\n",
    "PROGRESS_FILE = os.path.join(OUTPUT_DIR, \"progress.json\")\n",
    "\n",
    "# Maximum depth for scraping\n",
    "MAX_DEPTH = 1\n",
    "\n",
    "# Load or initialize progress tracking\n",
    "def load_progress():\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, \"r\") as file:\n",
    "            return json.load(file)\n",
    "    return {\"downloaded\": [], \"visited\": []}\n",
    "\n",
    "def save_progress(progress):\n",
    "    with open(PROGRESS_FILE, \"w\") as file:\n",
    "        json.dump(progress, file)\n",
    "\n",
    "def get_links(base_url, current_depth):\n",
    "    \"\"\"\n",
    "    Collects all unique links from the given page up to a specified depth.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the documentation.\n",
    "        current_depth (int): Current depth of scraping.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique absolute URLs found on the page.\n",
    "    \"\"\"\n",
    "    if current_depth > MAX_DEPTH:\n",
    "        return []\n",
    "\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch {base_url}: {response.status_code}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        url = urljoin(base_url, a_tag['href'])\n",
    "        if \"https://docs.manim.community/en/stable/\" in url:\n",
    "            links.append(url)\n",
    "    return list(set(links))  # Remove duplicates\n",
    "\n",
    "def make_links_absolute(html_content, base_url):\n",
    "    \"\"\"\n",
    "    Converts all relative links in the HTML content to absolute URLs.\n",
    "\n",
    "    Args:\n",
    "        html_content (str): The HTML content as a string.\n",
    "        base_url (str): The base URL to resolve relative links.\n",
    "\n",
    "    Returns:\n",
    "        str: The updated HTML content with absolute links.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    for tag in soup.find_all(['a', 'img'], href=True):\n",
    "        tag['href'] = urljoin(base_url, tag['href'])\n",
    "    for tag in soup.find_all('img', src=True):\n",
    "        tag['src'] = urljoin(base_url, tag['src'])\n",
    "    return str(soup)\n",
    "\n",
    "def download_page(url, output_dir, progress):\n",
    "    \"\"\"\n",
    "    Downloads the content of a given URL and saves it as an HTML file.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to download.\n",
    "        output_dir (str): The directory to save the HTML file.\n",
    "        progress (dict): The progress tracker dictionary.\n",
    "\n",
    "    Returns:\n",
    "        str: The file path of the saved HTML file, or None if the download fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Skip if already downloaded\n",
    "        if url in progress[\"downloaded\"]:\n",
    "            return None\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # Convert links to absolute for proper functionality\n",
    "            html_content = make_links_absolute(response.text, url)\n",
    "            file_name = url.split('/')[-1] or \"index.html\"\n",
    "            file_path = os.path.join(output_dir, f\"{file_name}.html\")\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(html_content)\n",
    "            progress[\"downloaded\"].append(url)\n",
    "            save_progress(progress)\n",
    "            return file_path\n",
    "        else:\n",
    "            print(f\"Failed to download {url}: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def html_to_pdf(html_files, output_pdf):\n",
    "    \"\"\"\n",
    "    Converts a list of HTML files into a single PDF file.\n",
    "\n",
    "    Args:\n",
    "        html_files (list): List of file paths to HTML files.\n",
    "        output_pdf (str): The path to the output PDF file.\n",
    "    \"\"\"\n",
    "    pdf_pages = []\n",
    "    for html_file in tqdm(html_files, desc=\"Rendering HTML to PDF\"):\n",
    "        try:\n",
    "            pdf_pages.append(HTML(html_file).render())\n",
    "        except Exception as e:\n",
    "            print(f\"Error rendering {html_file}: {e}\")\n",
    "    if pdf_pages:\n",
    "        combined_pdf = pdf_pages[0]\n",
    "        for page in pdf_pages[1:]:\n",
    "            combined_pdf.pages.extend(page.pages)\n",
    "        combined_pdf.write_pdf(output_pdf)\n",
    "        print(f\"Final PDF generated: {output_pdf}\")\n",
    "    else:\n",
    "        print(\"No PDF pages were generated.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to scrape the Manim documentation and export it as a PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Collect all relevant links\n",
    "        print(\"Collecting links from the documentation...\")\n",
    "        progress = load_progress()\n",
    "        if \"visited\" not in progress:\n",
    "            progress[\"visited\"] = []\n",
    "\n",
    "        links_to_visit = [BASE_URL]\n",
    "        all_links = set()\n",
    "\n",
    "        for depth in range(1, MAX_DEPTH + 1):\n",
    "            new_links = []\n",
    "            for link in links_to_visit:\n",
    "                if link not in progress[\"visited\"]:\n",
    "                    progress[\"visited\"].append(link)\n",
    "                    save_progress(progress)\n",
    "                    new_links.extend(get_links(link, depth))\n",
    "            all_links.update(new_links)\n",
    "            links_to_visit = new_links\n",
    "\n",
    "        print(f\"Found {len(all_links)} links within depth {MAX_DEPTH}.\")\n",
    "\n",
    "        # Step 2: Download all pages with a progress bar\n",
    "        print(\"Downloading pages...\")\n",
    "        html_files = []\n",
    "        for link in tqdm(all_links, desc=\"Downloading HTML pages\"):\n",
    "            file_path = download_page(link, OUTPUT_DIR, progress)\n",
    "            if file_path:\n",
    "                html_files.append(file_path)\n",
    "            time.sleep(0.5)  # Adjusted limit\n",
    "\n",
    "        # Step 3: Convert HTML files to a single PDF\n",
    "        print(\"Generating PDF...\")\n",
    "        pdf_path = os.path.join(OUTPUT_DIR, \"manim_docs_complete.pdf\")\n",
    "        html_to_pdf([os.path.join(OUTPUT_DIR, f) for f in os.listdir(OUTPUT_DIR) if f.endswith('.html')], pdf_path)\n",
    "\n",
    "        # Step 4: Handle output for Colab\n",
    "        if COLAB_ENV:\n",
    "            from google.colab import files\n",
    "            print(\"Downloading PDF to local machine...\")\n",
    "            files.download(pdf_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if COLAB_ENV:\n",
    "        print(\"Running in Google Colab environment.\")\n",
    "    else:\n",
    "        print(\"Running in local environment.\")\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create_Project_Estructure.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_project_structure(root_dir):\n",
    "    structure = {\n",
    "        \"assets\": [\"audio\", \"images\", \"videos\"],\n",
    "        \"docs\": [],\n",
    "        \"exports\": [\"reels\", \"shorts\", \"horizontal\"],\n",
    "        \"presets\": [],\n",
    "        \"scenes\": [\"examples\", \"storytelling\", \"educational\", \"transitions\"],\n",
    "        \"scripts\": [],\n",
    "        \"templates\": [],\n",
    "        \"tests\": [],\n",
    "        \"notebooks\": []  # Notebooks for development and exploration\n",
    "    }\n",
    "\n",
    "    files = {\n",
    "        \"docs\": [\"README.md\", \"SETUP.md\", \"USAGE.md\", \"ROADMAP.md\"],\n",
    "        \"presets\": [\"color_schemes.py\", \"transitions.py\", \"effects.py\", \"typography.py\"],\n",
    "        \"scripts\": [\"batch_render.py\", \"audio_sync.py\", \"video_export.py\", \"util.py\"],\n",
    "        \"templates\": [\"base_scene.py\", \"audio_scene.py\"],\n",
    "        \"tests\": [\"test_presets.py\", \"test_scenes.py\", \"test_utils.py\"],\n",
    "        \"notebooks\": [\"README.md\"],  # Added README for notebooks folder\n",
    "        \"root\": [\".gitignore\", \"config.py\", \"requirements.txt\", \"run.py\"]\n",
    "    }\n",
    "\n",
    "    comments = {\n",
    "        \"assets\": \"# Media files for animations (audio, images, videos)\",\n",
    "        \"docs\": \"# Documentation of the project\",\n",
    "        \"exports\": \"# Exported videos (reels, shorts, horizontal)\",\n",
    "        \"notebooks\": \"# Notebooks for development and exploration\",\n",
    "        \"presets\": \"# Reusable visual effects and style configurations\",\n",
    "        \"scenes\": \"# Manim scenes (examples, storytelling, etc.)\",\n",
    "        \"scripts\": \"# Automation tools and utility functions\",\n",
    "        \"templates\": \"# Base classes and reusable components for scenes\",\n",
    "        \"tests\": \"# Unit tests for ensuring stability\"\n",
    "    }\n",
    "\n",
    "    # Create root directory\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "    # Create folders and subfolders\n",
    "    for folder, subfolders in structure.items():\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        for subfolder in subfolders:\n",
    "            os.makedirs(os.path.join(folder_path, subfolder), exist_ok=True)\n",
    "\n",
    "    # Create files in specific folders\n",
    "    for folder, filenames in files.items():\n",
    "        target_dir = root_dir if folder == \"root\" else os.path.join(root_dir, folder)\n",
    "\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(target_dir, filename)\n",
    "            with open(file_path, \"w\") as f:\n",
    "                if filename.endswith(\".md\"):\n",
    "                    f.write(f\"# {filename.split('.')[0]}\\n\")  # Basic header for Markdown files\n",
    "                elif filename == \".gitignore\":\n",
    "                    f.write(\"# Ignore files\\n*.pyc\\n__pycache__/\\n\")\n",
    "                else:\n",
    "                    f.write(\"# Placeholder content\\n\")\n",
    "\n",
    "    print(f\"Project structure created at: {root_dir}\")\n",
    "\n",
    "def create_directory_tree_txt(path, output_file=None):\n",
    "    if output_file is None:\n",
    "        output_file = os.path.join(path, \"directory_tree.txt\")\n",
    "\n",
    "    comments = {\n",
    "        \"assets\": \"# Media files for animations (audio, images, videos)\",\n",
    "        \"docs\": \"# Documentation of the project\",\n",
    "        \"exports\": \"# Exported videos (reels, shorts, horizontal)\",\n",
    "        \"notebooks\": \"# Notebooks for development and exploration\",\n",
    "        \"presets\": \"# Reusable visual effects and style configurations\",\n",
    "        \"scenes\": \"# Manim scenes (examples, storytelling, etc.)\",\n",
    "        \"scripts\": \"# Automation tools and utility functions\",\n",
    "        \"templates\": \"# Base classes and reusable components for scenes\",\n",
    "        \"tests\": \"# Unit tests for ensuring stability\"\n",
    "    }\n",
    "\n",
    "    def generate_tree(path, prefix=\"\"):\n",
    "        entries = os.listdir(path)\n",
    "        entries.sort()\n",
    "        lines = []\n",
    "        for index, entry in enumerate(entries):\n",
    "            full_path = os.path.join(path, entry)\n",
    "            connector = \"└── \" if index == len(entries) - 1 else \"├── \"\n",
    "            comment = f\" {comments.get(entry, '')}\" if entry in comments else \"\"\n",
    "            lines.append(f\"{prefix}{connector}{entry}{comment}\")\n",
    "            if os.path.isdir(full_path):\n",
    "                extension = \"    \" if index == len(entries) - 1 else \"│   \"\n",
    "                lines.extend(generate_tree(full_path, prefix + extension))\n",
    "        return lines\n",
    "\n",
    "    tree_lines = generate_tree(path)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(tree_lines))\n",
    "\n",
    "    print(f\"Directory tree saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project structure created at: C:\\Users\\User\\Projects_Unprotected\\Media_Generators\\notebooks\n",
      "Directory tree saved to C:\\Users\\User\\Projects_Unprotected\\Media_Generators\\notebooks\\directory_tree.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = r\"C:\\Users\\User\\Projects_Unprotected\\Media_Generators\"\n",
    "    create_project_structure(root_dir)\n",
    "    create_directory_tree_txt(root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project structure created at: C:\\Users\\User\\Projects_Unprotected\\Media_Generators\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "# Replace \"your_project_directory\" with the desired path for the project\n",
    "create_project_structure(r\"C:\\Users\\User\\Projects_Unprotected\\Media_Generators\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory tree saved to C:\\Users\\User\\Projects_Unprotected\\Media_Generators\\directory_tree.txt\n"
     ]
    }
   ],
   "source": [
    "create_directory_tree_txt(r\"C:\\Users\\User\\Projects_Unprotected\\Media_Generators\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiguelEnvHaB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
